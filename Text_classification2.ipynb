{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTING REQUIRED LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import re\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRACTING DATA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all data from tar file (20_newsgroups.tar.gz) to a folder (Documents_data)\n",
    "#import tarfile\n",
    "#my_tar = tarfile.open('20_newsgroups.tar.gz')\n",
    "#my_tar.extractall('./Documents_data') # specify which folder to extract to\n",
    "#my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLLECTING DATA IN  'X'  AND  'Y'  FORM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =[] \n",
    "Y = []\n",
    "for category in os.listdir(\"Documents_data/20_newsgroups\"):\n",
    "    for document in os.listdir(\"Documents_data/20_newsgroups/\"+category):\n",
    "        with open(\"Documents_data/20_newsgroups/\"+category+'/'+document, \"r\") as f:\n",
    "            X.append(f.read())\n",
    "            Y.append(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADDING STOP WORDS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\n",
    "\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\",\n",
    "\"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \n",
    "\"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \n",
    "\"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \n",
    "\"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \n",
    "\"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\",\n",
    "\"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \n",
    "\"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \n",
    "\"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \n",
    "\"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \n",
    "\"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\",\n",
    "\"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\",\n",
    "\"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \n",
    "\"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \n",
    "\"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\",\n",
    "\"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLITING DATA INTO TRAINING AND TASTING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUILDING VOCABULARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93499\n"
     ]
    }
   ],
   "source": [
    "# Going through all the documents and within a document we are going through each word\n",
    "# Building a dictionary vocab with the words and their counts\n",
    "# We will not include stop words and words whose length is less than 3\n",
    "'''\n",
    "    \\W -> Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
    "'''\n",
    "vocab = {}\n",
    "for doc in x_train:\n",
    "    doc= doc.lower()\n",
    "    stripped=re.split(r'\\W+',doc)\n",
    "    for word in stripped :\n",
    "        if(word.isalpha()) and (len(word)>2) and (word not in stop_words) :\n",
    "            vocab[word] = vocab.get(word,0) + 1\n",
    "            #print(word,end =\" \")\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUILDING VOCABULARY WITH TOP 2000 WORDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are considering only top 2000 words in our vocabulary because as we are taking more words after 2000 their frequency is \n",
    "#very less as you can see in below plot and if we create training matrix using that column corresponding \n",
    "#to this word will contain mostly zeros. So no need to add furher words bcz they will not affect our classification algorithm.\n",
    "\n",
    "sorted_tuples = sorted(vocab.items(), key=lambda item: item[1],reverse = True)\n",
    "vocab = {k: v for k, v in sorted_tuples}\n",
    "df = pd.DataFrame(list(vocab.items()))\n",
    "freq = df[1]\n",
    "words_count = list(df.index)\n",
    "plt.figure(figsize =(9, 4))\n",
    "plt.axis([0,8000, 1, 4000])\n",
    "plt.plot(words_count, freq)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# final vocabulary is dictionary it will contain all top 2000 words as key and an index corresponding to each keys\n",
    "final_vocab ={}\n",
    "index = 0\n",
    "for i , j in vocab.items() :\n",
    "    final_vocab[i] = index\n",
    "    index += 1\n",
    "    if index == 2000 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING X_TRAIN , Y_TRAIN DATASET**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14997, 2000)\n",
      "14997\n"
     ]
    }
   ],
   "source": [
    "# creating x_train datset with 14997 rows for each document and 2000 column for each word\n",
    "# doc_no will be row number and value in final_vocab dictionary will be column number\n",
    "# In y train row number will be the output of corresponding row of x_train\n",
    "doc_no = 0\n",
    "X_train = np.full((len(x_train),2000),0)\n",
    "Y_train = np.empty(len(y_train), dtype='<U30')\n",
    "for doc in x_train:\n",
    "    doc= doc.lower()\n",
    "    stripped=re.split(r'\\W+',doc)\n",
    "    for word in stripped :\n",
    "        if final_vocab.get(word,None) is not None :\n",
    "              idx = final_vocab[word]\n",
    "              X_train[doc_no][idx] += 1\n",
    "    Y_train[doc_no] = y_train[doc_no]\n",
    "    doc_no += 1\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING X_TEST , Y_TEST DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2000)\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "doc_no = 0\n",
    "X_test = np.full((len(x_test),2000),0)\n",
    "Y_test = np.empty(len(y_test), dtype='<U30')\n",
    "for doc in x_test:\n",
    "    doc= doc.lower()\n",
    "    stripped=re.split(r'\\W+',doc)\n",
    "    for word in stripped :\n",
    "        if final_vocab.get(word,None) is not None :\n",
    "              idx = final_vocab[word]\n",
    "              X_test[doc_no][idx] += 1\n",
    "    Y_test[doc_no] = y_test[doc_no]\n",
    "    doc_no += 1\n",
    "    \n",
    "print(X_test.shape)\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1990  \\\n",
       "0     9     2     3     1     2     2     4     1     1     1  ...      0   \n",
       "1     4     1     4     2     1     1     1     1     1     1  ...      0   \n",
       "2     2     2     8     0     2     2     1     1     1     1  ...      0   \n",
       "3    12     3     0     1     1     1     1     1     1     1  ...      0   \n",
       "4     6     3     0     1     1     1     0     1     1     1  ...      0   \n",
       "\n",
       "   1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how our data set look like\n",
    "# columns numbers represent indexes corresponding to all diff 2000 words\n",
    "#in dictionary\n",
    "# each row number represents a document\n",
    "# and the value stored at (i,j) is count of jth word in document i\n",
    "df = pd.DataFrame(X_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTINOMIAL NAIVE BAYES ML MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STRUCTURE OF RESULT DICTIONARY**\n",
    "\n",
    "**result**  -> total data ,  vocab_size  ,**class_1 ,  class_2 ,  class_3  ... class_n**\n",
    "\n",
    "**class_1** -> current_class_count  ,total_count,   word1(feature1),   word2(feature2)   , ...... word2000(feature200)\n",
    "\n",
    "\"total_data\" is total number of documnets.\n",
    "\n",
    "\"vocab_size\" is total words present in vocabulary\n",
    "\n",
    "\"current_class_count\" is total number of documents in class_1\n",
    "\n",
    "\"total_count\" is count of total words in class_1\n",
    "\n",
    "\"word1(feature1)\" is count of word_1 in class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit fnction returns a dictionary\n",
    "def fit(X_train, Y_train):\n",
    "    count = {}\n",
    "    vocab_size = 0  # total number of words present in Vocab\n",
    "    count[\"total_data\"] = len(Y_train) # total number of documents\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        count[current_class] = {}\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        \n",
    "        # Get X_train corresponding to current_class\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        \n",
    "        # total no. of documents of current_class\n",
    "        count[current_class][\"current_class_count\"] = len(Y_train_current)\n",
    "        \n",
    "        # Iterate over all words and store its frequency\n",
    "        num_features = X_train.shape[1]\n",
    "        total_current_class_words = 0\n",
    "        for j in range(1, num_features + 1):\n",
    "            # store frequency of j th word(feature) corresponding to current_class \n",
    "            count[current_class][j] = X_train_current[:, j-1].sum()\n",
    "            total_current_class_words += X_train_current[:, j-1].sum()\n",
    "            \n",
    "        # store total words of a current_class also\n",
    "        count[current_class][\"total_count\"] = total_current_class_words\n",
    "        vocab_size += total_current_class_words\n",
    "    count[\"vocab_size\"] = vocab_size\n",
    "    return count # returns dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    #  log( P(y=current_class) )\n",
    "    output = np.log(dictionary[current_class]['current_class_count']) - np.log(dictionary[\"total_data\"]) \n",
    "    \n",
    "    num_features = len(dictionary[current_class].keys()) - 2  # -2 bcz 2 keys are total_count and current_class_count\n",
    "    for j in range(1,num_features+1):\n",
    "        xj = x[j-1]\n",
    "        if xj == 0: # word not in test data document\n",
    "            continue\n",
    "            \n",
    "        '''\n",
    "            word is present as a feature. So add its corresponding logarithmic probability\n",
    "            Also use laplace correction i.e\n",
    "            log( P(w1|y=current_class) ) = log( count of all w1 corresponding \n",
    "                                                to current_class + 1)\n",
    "                                                      -\n",
    "                                            log( count of all words of current_class\n",
    "                                                 + size of vocabulary)\n",
    "        '''  \n",
    "        count_jth_word_in_current_class = dictionary[current_class][j] + 1  # +1 bcz of laplace correction\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + dictionary[\"vocab_size\"]\n",
    "        count_xj_probability = np.log(count_jth_word_in_current_class) - np.log(count_current_class)\n",
    "        output = output + count_xj_probability\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000   # Initially\n",
    "    best_class = -1\n",
    "    first_run = True  # Bcz best_p should be changed for the first time\n",
    "    for current_class in classes:\n",
    "        if current_class == \"vocab_size\" or current_class==\"total_data\":\n",
    "            continue\n",
    "            \n",
    "        # Find logarithmic probability of current_class\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False  # bcz we done for 1st time\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding predictions using our own Multionomial classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(dictionary, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "   10  45]\n",
      " [  0 184   2   7   0  34   0   0   0   0   0  17   0   0   1   0   0   0\n",
      "    8   0]\n",
      " [  0  23 112   9   0  76   0   0   0   0   0  18   0   0   1   0   1   0\n",
      "    9   0]\n",
      " [  0   8   1 204   3   8   2   0   0   0   0  10   0   0   0   0   0   0\n",
      "    4   0]\n",
      " [  0   6   1  42 152  15   2   0   0   0   0  10   1   0   0   0   1   1\n",
      "    5   0]\n",
      " [  0   8   4   3   0 218   1   0   0   0   0   4   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [  0   1   1  18   0   1 180   3   0   0   0   6   7   0   2   0   1   0\n",
      "   41   0]\n",
      " [  0   1   0   0   0   1   5 191   0   0   1   2   3   1   1   0  24   2\n",
      "   37   0]\n",
      " [  4   1   0   0   0   0   1  11 165   0   0   8   0   1   0   0  25   2\n",
      "   66   0]\n",
      " [  0   0   0   0   0   0   0   0   0 190  26   0   0   0   1   0   1   1\n",
      "   29   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 221   0   0   0   0   0   1   0\n",
      "    9   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0 229   0   0   0   0   0   0\n",
      "    3   0]\n",
      " [  0   6   0   3   0   0   0   0   0   1   0  55 137   2   7   0   2   2\n",
      "   28   1]\n",
      " [  0   4   0   0   0   1   0   0   0   0   0   9   0 172   2   0   1   3\n",
      "   59   5]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   6   0   0 205   0   2   1\n",
      "   31   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 251   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0 157   2\n",
      "   85   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 265\n",
      "   16   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   7  14\n",
      "  232   5]\n",
      " [ 36   0   0   0   0   0   0   0   0   0   0   2   0   0   0   3   7   5\n",
      "   50 133]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.76      0.78       233\n",
      "           comp.graphics       0.75      0.73      0.74       253\n",
      " comp.os.ms-windows.misc       0.93      0.45      0.61       249\n",
      "comp.sys.ibm.pc.hardware       0.71      0.85      0.78       240\n",
      "   comp.sys.mac.hardware       0.98      0.64      0.78       236\n",
      "          comp.windows.x       0.61      0.91      0.73       240\n",
      "            misc.forsale       0.94      0.69      0.80       261\n",
      "               rec.autos       0.93      0.71      0.81       269\n",
      "         rec.motorcycles       1.00      0.58      0.73       284\n",
      "      rec.sport.baseball       0.99      0.77      0.87       248\n",
      "        rec.sport.hockey       0.89      0.96      0.92       231\n",
      "               sci.crypt       0.60      0.98      0.75       233\n",
      "         sci.electronics       0.93      0.56      0.70       244\n",
      "                 sci.med       0.98      0.67      0.80       256\n",
      "               sci.space       0.93      0.83      0.88       246\n",
      "  soc.religion.christian       0.99      1.00      0.99       252\n",
      "      talk.politics.guns       0.68      0.63      0.66       249\n",
      "   talk.politics.mideast       0.89      0.94      0.91       281\n",
      "      talk.politics.misc       0.32      0.90      0.47       259\n",
      "      talk.religion.misc       0.69      0.56      0.62       236\n",
      "\n",
      "             avg / total       0.83      0.75      0.77      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Inbuilt Multionomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[195   0   0   0   0   0   0   4   1   0   0   0   0   1   0   1   0   0\n",
      "    0  31]\n",
      " [  0 203  12  21   3   8   2   2   0   1   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   8 195  19   5  14   3   1   0   0   0   0   2   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   3 196  32   0   5   0   1   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0  23 205   0   5   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  17  18  14   9 169   3   0   2   1   0   1   3   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   7   3   0 237   5   1   0   1   0   5   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   1   0  11 247   3   1   1   0   4   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   4 277   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   4   2 227  12   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   2   7 221   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   0   0   2   1   0   0   0   0 224   2   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   2   0  10   3   0   2   7   2   1   0   0 214   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   2   6   1   1   3   9   3   0   1   3 219   4   0   0   1\n",
      "    0   0]\n",
      " [  0   1   0   1   0   0   1   2   2   2   1   0   4   3 227   0   1   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 252   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   0   3   0   0   3   0   0   0   0 223   0\n",
      "   14   5]\n",
      " [  0   0   0   1   1   0   3   3   1   2   1   0   0   1   1   0   6 238\n",
      "   21   2]\n",
      " [  0   0   0   0   0   0   1   0   0   1   0   4   0   3   2   2  33  15\n",
      "  177  21]\n",
      " [ 45   1   0   0   0   0   1   0   0   0   1   0   1   1   0  10  11   2\n",
      "   21 142]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.84      0.82       233\n",
      "           comp.graphics       0.84      0.80      0.82       253\n",
      " comp.os.ms-windows.misc       0.86      0.78      0.82       249\n",
      "comp.sys.ibm.pc.hardware       0.66      0.82      0.73       240\n",
      "   comp.sys.mac.hardware       0.76      0.87      0.81       236\n",
      "          comp.windows.x       0.87      0.70      0.78       240\n",
      "            misc.forsale       0.85      0.91      0.88       261\n",
      "               rec.autos       0.88      0.92      0.90       269\n",
      "         rec.motorcycles       0.91      0.98      0.94       284\n",
      "      rec.sport.baseball       0.92      0.92      0.92       248\n",
      "        rec.sport.hockey       0.93      0.96      0.94       231\n",
      "               sci.crypt       0.96      0.96      0.96       233\n",
      "         sci.electronics       0.89      0.88      0.88       244\n",
      "                 sci.med       0.94      0.86      0.89       256\n",
      "               sci.space       0.94      0.92      0.93       246\n",
      "  soc.religion.christian       0.95      1.00      0.97       252\n",
      "      talk.politics.guns       0.81      0.90      0.85       249\n",
      "   talk.politics.mideast       0.93      0.85      0.89       281\n",
      "      talk.politics.misc       0.76      0.68      0.72       259\n",
      "      talk.religion.misc       0.71      0.60      0.65       236\n",
      "\n",
      "             avg / total       0.86      0.86      0.86      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between inbuilt and our own builtN classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From above results we see that inbuilt Multinomial naive bayes has acuuracy of 86%**\n",
    "\n",
    "\n",
    "**Where as accuracy achieved using our own Multinomial classifier is 75%**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
